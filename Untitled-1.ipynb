{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Training Corpus (The Big Corpus)\n",
    "training_corpus = nltk.corpus.brown.tagged_sents(tagset=\"universal\")[:10000]\n",
    "training_corpus_lower = [[(word.lower(), tag) for word, tag in sent] for sent in training_corpus]\n",
    "trainlist = []\n",
    "for i in training_corpus_lower:\n",
    "    for j in i:\n",
    "        trainlist.append(j)\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "# Grabbing words\n",
    "corplist_words = [(tag[0]) for tag in trainlist]\n",
    "\n",
    "# Grabbing tag\n",
    "corplist_tag = [tag[1] for tag in trainlist]\n",
    "\n",
    "# Checking unique words in corpus\n",
    "corplist_words_set = list(set(corplist_words))\n",
    "corplist_words_set.append(\"UNK\")  # Adding Unknown Words\n",
    "\n",
    "# Checking unique tag in corpus\n",
    "corplist_tag_set = list(set(corplist_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Test Sentences\n",
    "test_corpus = nltk.corpus.brown.tagged_sents(tagset=\"universal\")[10150:10151]\n",
    "test_corpus_lower = [[(word.lower(), tag) for word, tag in sent] for sent in test_corpus]\n",
    "testlist = []\n",
    "for a in test_corpus_lower:\n",
    "    for b in a:\n",
    "        testlist.append(b)\n",
    "        pass\n",
    "    pass\n",
    "testlist_words = [tag[0] for tag in testlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editing Input from string to number | Based on unique list of words\n",
    "numbered_input = []\n",
    "for word in testlist_words:\n",
    "    if word not in corplist_words_set:\n",
    "        numbered_input.append(len(corplist_words_set))\n",
    "        pass\n",
    "    else:\n",
    "        numbered_input.append((corplist_words_set.index(word)))\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to help emission / observation matrix\n",
    "def word_tag(w1, w2, corpus=trainlist):\n",
    "    dictword = []\n",
    "    dictall = []\n",
    "    for i in corpus:\n",
    "        if i[0] == w1:\n",
    "            dictall.append(i)\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        pass\n",
    "    else:\n",
    "        for j in dictall:\n",
    "            if j[1] == w2:\n",
    "                dictword.append(j)\n",
    "                pass\n",
    "            else:\n",
    "                pass\n",
    "            pass\n",
    "    return (len(dictword), len(dictall))\n",
    "\n",
    "# Create the emission matrix\n",
    "words_matrix = np.ones((len(corplist_words_set), len(corplist_tag_set)), dtype='float32')\n",
    "\n",
    "# Call the actual word based on index\n",
    "real_words = []\n",
    "for item in numbered_input:\n",
    "    if item == len(corplist_words_set):\n",
    "        item = int(item) - 1\n",
    "        real_words.append(corplist_words_set[item])\n",
    "        pass\n",
    "    else:\n",
    "        real_words.append(corplist_words_set[item])\n",
    "\n",
    "# Creating Emission Matrix\n",
    "for a, t1 in enumerate(corplist_words_set):\n",
    "    for b, t2 in enumerate(corplist_tag_set):\n",
    "        if word_tag\n",
    "        words_matrix[a, b] = word_tag(t1, t2)[0] / word_tag(t1, t2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DET</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>.</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADV</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "      <th>PRON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>those</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coming</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denominations</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>welcome</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opportunity</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>become</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informed</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               DET       ADJ       ADP    .       PRT  ADV      NOUN  CONJ  \\\n",
       "those          1.0  0.000000  0.000000  0.0  0.000000  0.0  0.000000   0.0   \n",
       "coming         0.0  0.000000  0.000000  0.0  0.000000  0.0  0.093750   0.0   \n",
       "from           0.0  0.000000  1.000000  0.0  0.000000  0.0  0.000000   0.0   \n",
       "other          0.0  1.000000  0.000000  0.0  0.000000  0.0  0.000000   0.0   \n",
       "denominations  1.0  1.000000  1.000000  1.0  1.000000  1.0  1.000000   1.0   \n",
       "will           0.0  0.000000  0.000000  0.0  0.000000  0.0  0.025388   0.0   \n",
       "welcome        0.0  0.384615  0.000000  0.0  0.000000  0.0  0.230769   0.0   \n",
       "the            1.0  0.000000  0.000000  0.0  0.000000  0.0  0.000000   0.0   \n",
       "opportunity    0.0  0.000000  0.000000  0.0  0.000000  0.0  1.000000   0.0   \n",
       "to             0.0  0.000000  0.417078  0.0  0.582716  0.0  0.000206   0.0   \n",
       "become         0.0  0.000000  0.000000  0.0  0.000000  0.0  0.000000   0.0   \n",
       "informed       0.0  0.000000  0.000000  0.0  0.000000  0.0  0.000000   0.0   \n",
       ".              0.0  0.000000  0.000000  1.0  0.000000  0.0  0.000000   0.0   \n",
       "\n",
       "               NUM      VERB    X  PRON  \n",
       "those          0.0  0.000000  0.0   0.0  \n",
       "coming         0.0  0.906250  0.0   0.0  \n",
       "from           0.0  0.000000  0.0   0.0  \n",
       "other          0.0  0.000000  0.0   0.0  \n",
       "denominations  1.0  1.000000  1.0   1.0  \n",
       "will           0.0  0.974612  0.0   0.0  \n",
       "welcome        0.0  0.384615  0.0   0.0  \n",
       "the            0.0  0.000000  0.0   0.0  \n",
       "opportunity    0.0  0.000000  0.0   0.0  \n",
       "to             0.0  0.000000  0.0   0.0  \n",
       "become         0.0  1.000000  0.0   0.0  \n",
       "informed       0.0  1.000000  0.0   0.0  \n",
       ".              0.0  0.000000  0.0   0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df = pd.DataFrame(words_matrix, columns = list(corplist_tag_set), index=list(testlist_words))\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 9.3750000e-02, 0.0000000e+00,\n",
       "        0.0000000e+00, 9.0625000e-01, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 2.5387870e-02, 0.0000000e+00,\n",
       "        0.0000000e+00, 9.7461212e-01, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 3.8461539e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 2.3076923e-01, 0.0000000e+00,\n",
       "        0.0000000e+00, 3.8461539e-01, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 4.1707820e-01, 0.0000000e+00,\n",
       "        5.8271605e-01, 0.0000000e+00, 2.0576132e-04, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to help transition matrix\n",
    "def t2_given_t1(t2, t1, corpus=trainlist):\n",
    "    tags = []\n",
    "    for i in corpus:\n",
    "        tags.append(i[1])\n",
    "    count_t1 = len([t for t in tags if t == t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags) - 1):\n",
    "        if tags[index] == t1 and tags[index + 1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)\n",
    "\n",
    "# Making pos matrix\n",
    "tags_matrix = np.ones((len(corplist_tag_set), len(corplist_tag_set)), dtype=\"float32\")\n",
    "for i, t1 in enumerate(corplist_tag_set):\n",
    "    for j, t2 in enumerate(corplist_tag_set):\n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0] / t2_given_t1(t2, t1)[1]\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to help pi\n",
    "def help_pi(pi1, raw_corpus=training_corpus_lower):\n",
    "    adding = 0\n",
    "    for items in raw_corpus:\n",
    "        if items[0][1] == pi1:\n",
    "            adding += 1\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        pass\n",
    "    return adding/len(raw_corpus)\n",
    "\n",
    "# Create Initial State Prob\n",
    "pi = np.ones(len(corplist_tag_set), dtype='float32')\n",
    "num = 0\n",
    "for item in corplist_tag_set:\n",
    "    pi[num] = help_pi(item)\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple, TypeVar\n",
    "import numpy as np\n",
    "Q = TypeVar(\"Q\")\n",
    "V = TypeVar(\"V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ndarray[Tuple[V]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
